{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T09:25:06.462154Z",
     "start_time": "2023-04-19T09:25:05.043600Z"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter optimization with Bayesian optimization\n",
    "\n",
    "Bayesian optimization (BO) is a tool to optimize black-box function, i.e., functions with unknown structure that are expensive to evaluate.\n",
    "BO consists of the following steps that are repeated until one runs out of time or money:\n",
    "\n",
    "1. Learn a model of the function\n",
    "2. Using this model, find a promising point to evaluate next\n",
    "3. Evaluate the point\n",
    "4. Refine the model using the new observation\n",
    "\n",
    "In this lab, we will first learn the BO concepts on a simple example to understand the inner workings and then transfer this to a real-world hyperparameter optimization task to see how you would implement it in practice. \n",
    "We will use Gaussian processes (GPs) as surrogate models.\n",
    "Together with random forests (RFs), GPs are the most popular type of surrogate model.\n",
    "Generally, a surrogate model for BO must provide an uncertainty estimate, i.e., for a given point, the model should be able to say how certain it is in its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.567081244Z",
     "start_time": "2023-05-08T11:54:02.564786687Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first define a synthetic benchmark function we will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.793540907Z",
     "start_time": "2023-05-08T11:54:02.567525961Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fn(\n",
    "        x: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    y = np.sin(x) + np.sin((10.0 / 3.0) * x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# define lower and upper bounds of the function: we will only search for the optimum within these bounds\n",
    "lb, ub = -2.7, 7.5\n",
    "\n",
    "plt.plot(np.linspace(lb, ub, 200), fn(np.linspace(lb, ub, 200)), label='f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the training and test data\n",
    "(This will be used further down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.849967898Z",
     "start_time": "2023-05-08T11:54:02.795115052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.random.RandomState(1).rand(5) * (ub - lb) + lb\n",
    "x_train = x_train[:, np.newaxis]\n",
    "y_train = fn(x_train)\n",
    "\n",
    "x_test = np.linspace(lb, ub, 250)[:, np.newaxis]\n",
    "y_test = fn(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Gaussian process\n",
    "\n",
    "A GP can be seen as a probability distribution over _functions_.\n",
    "One can draw functions from a GP and given some observations, one can update the probability distribution using Bayes' rule.\n",
    "In particular, we assume that our function $f$ is distributed according to a GP:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}(\\mu(\\mathbf{x}), k(\\mathbf{x},\\mathbf{x'})).\n",
    "$$\n",
    "\n",
    "$\\mu$ is the _mean function_ and $k$ is the _kernel_ or _covariance function_. \n",
    "The mean function describes the average value of a point and the covariance function essentially captures how much two points are, on average, correlated.\n",
    "\n",
    "In our example, we will use the RBF kernel:\n",
    "\n",
    "$$\n",
    " k(\\mathbf{x},\\mathbf{x'}) = \\exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{x}'||^2}{2l^2}\\right).\n",
    "$$\n",
    "\n",
    "This kernel-function has a larger value the closer $\\mathbf{x}$ and $\\mathbf{x'}$ are to each other, i.e., it assumes that the relation of two points is only determined by their distance. \n",
    "There are other kernels whose value depends on the location of the points but we will only work with kernels that have this \"distance\" assumption.\n",
    "The parameter $l$ is a hyperparameter of the model.\n",
    "We will see later how it affects the GP and we will see how it can be learned from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an RBF kernel with a global lengthscale\n",
    "\n",
    "**Your task:** Implement the RBF kernel as described above.\n",
    "\n",
    "**Question:** What does a value of 1 mean, what does a value of 0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.850280600Z",
     "start_time": "2023-05-08T11:54:02.838935547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kern(x: np.ndarray, x_prime: np.ndarray, ls: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the covariance matrix for x and x'. \n",
    "    \n",
    "    Note: the output should always be a 2d matrix. If x or x' are 1d vectors (with d elements), we first reshape them to (1,d)-matrices.\n",
    "    If x is of shape (n,d) and x' is of shape (m,d), the output will be a (n x m) matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim == 1:\n",
    "        x = x[np.newaxis, :]\n",
    "    if x_prime.ndim == 1:\n",
    "        x_prime = x_prime[np.newaxis, :]\n",
    "        \n",
    "    # ➡️ TODO : implement the RBF kernel ⬅️\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety check\n",
    "\n",
    "The cell below should display the following image:\n",
    "\n",
    "![RBF kernel plot](kernel_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.312165140Z",
     "start_time": "2023-05-08T11:54:02.839126389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRANULARITY = 50\n",
    "\n",
    "x = np.linspace(-2, 2, GRANULARITY).reshape(-1, 1)\n",
    "y = np.linspace(-2, 2, GRANULARITY).reshape(-1, 1)\n",
    "\n",
    "xlabels = [''] * GRANULARITY\n",
    "ylabels = [''] * GRANULARITY\n",
    "\n",
    "for idx in np.arange(GRANULARITY)[::5]:\n",
    "    xlabels[idx] = f\"{x.squeeze()[idx]:.2f}\"\n",
    "    ylabels[idx] = f\"{y.squeeze()[idx]:.2f}\"\n",
    "\n",
    "z = kern(x, y, 0.5)\n",
    "assert z.ndim == 2, \"kern must be a two dimensional matrix\"\n",
    "assert z.shape == (x.shape[0], x.shape[0]), f\"kern should in this case yield a {x.shape[0]} by {x.shape[0]} matrix\"\n",
    "sns.heatmap(z, xticklabels=xlabels, yticklabels=ylabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a mean function (constant zero)\n",
    "\n",
    "Surprisingly, the mean function is not really important. \n",
    "All we are interested in is the posterior of the GP, i.e., the distribution over functions after we have seen some data.\n",
    "We will see that the prior mean (which is the mean function here) is unimportant for that.\n",
    "Therefore, we can just implement the mean function as constant zero. This is especially true, as we tend to standardize our data so that is is zero mean.\n",
    "\n",
    "**Your task:** Implement the mean function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.317176932Z",
     "start_time": "2023-05-08T11:54:03.314652945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the mean. \n",
    "    \n",
    "    If x is a 1d vector with d elements, reshape it to a (1 x d) matrix first.\n",
    "    Should return a 2d zeros vector.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # ➡️ TODO : reshape to 2d ⬅️\n",
    "    \n",
    "    # ➡️ TODO : implement the mean function ⬅️\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety check\n",
    "\n",
    "The output of the following cell should be __exactly__ `array([[0.]]) (1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.411050475Z",
     "start_time": "2023-05-08T11:54:03.318046991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mean(np.zeros((1, 4))), mean(np.zeros(4)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GP implementation\n",
    "\n",
    "We will now use the mean and covariance functions to build a GP. \n",
    "In the `__call__` function, we will compute the posterior distribution for a vector of points `x`. The output will __not__ be an array, but a __multivariate Gaussian__ with the dimensionality of the length of `x`.\n",
    "\n",
    "The posterior mean $\\mu_n$ of a GP is calculated as follows:\n",
    "$$\n",
    "\\mu_n(\\mathbf{x}) = \\Sigma_0(\\mathbf{x},\\mathbf{x}_{1:n})\\Sigma_0(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}(f(\\mathbf{x}_{1:n})-\\mu(\\mathbf{x}_{1:n}))+\\mu(\\mathbf{x})\n",
    "$$\n",
    "where $\\Sigma_{i,j}=k(\\mathbf{x}_i,\\mathbf{x}_j)$ is the kernel matrix or Gram matrix. $\\mathbf{x}_{1:n}$ is the training data for which we have observed function values and $\\mathbf{x}$ is the point for which we want to make predictions.\n",
    "\n",
    "The posterior variance is calculated as \n",
    "$$\n",
    "\\sigma_n(\\mathbf{x}) = k(\\mathbf{x},\\mathbf{x})-\\Sigma_0(\\mathbf{x},\\mathbf{x}_{1:n})\\Sigma_0(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}\\Sigma_0(\\mathbf{x},\\mathbf{x}_{1:n})^{\\intercal}\n",
    "$$\n",
    "\n",
    "In the `fit`  function, we will set the training data and calculate a critical component: the inverse of the covariance matrix $\\Sigma_0(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}$.\n",
    "Usually, this is done using Cholesky decomposition but we will use `np.linalg.inv` to calculate the inverse.\n",
    "\n",
    "Pro tip: You might run into problems when computing the inverse due to numerical instabilities. If this happens, add a value of $10^{-6}$ to the diagonal elements of the matrix before computing the inverse.\n",
    "\n",
    "**Your task:** Set the training data and compute the inverse of the covariance matrix $\\Sigma_0(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.426815886Z",
     "start_time": "2023-05-08T11:54:03.363421012Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(\n",
    "            self,\n",
    "            mean_function: Callable[[np.ndarray], np.ndarray],\n",
    "            kern_function: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "            ls: float\n",
    "    ):\n",
    "        # setting the mean function\n",
    "        self.mean = mean_function\n",
    "        # setting the kernel function, fix the lengthscale so we don't have to pass it all the time\n",
    "        self.kern = lambda x, y: kern_function(x, y, ls)\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def initialize(self, x_train: np.ndarray, y_train: np.ndarray) -> 'GaussianProcess':\n",
    "        # some checks\n",
    "        assert x_train.ndim == 2 and y_train.ndim == 2, 'x_train and y_train should be 2D arrays'\n",
    "        assert x_train.shape[0] == y_train.shape[0], 'first dimension of x_train and y_train has to be equal (n_points)'\n",
    "        assert y_train.shape[1] == 1, 'y_train has to be of form (n_points, 1)'\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        # Due to numerical instabilities, the covariance matrix might not be invertible.\n",
    "        # We add a small constant value to the diagonal elements ('jitter') to enfore the\n",
    "        # matrix to be positive semidefinite (which implies invertibility)\n",
    "        # See, e.g., https://scicomp.stackexchange.com/questions/36342/advantage-of-diagonal-jitter-for-numerical-stability\n",
    "        #\n",
    "        #\n",
    "        # The inputs x_train and y_train both have to be 2D here. x_train has shape (n_points, dim_of_points), y_train has\n",
    "        # shape (n_points, 1)\n",
    "        #\n",
    "        # ➡️ TODO : compute the covariance and the inverse of the covariance matrix here. save them as class attributes so we don't need to\n",
    "        #  recompute them all the time ⬅️\n",
    "        #\n",
    "        self.cov = ...\n",
    "        self.cov_inv = ...\n",
    "        return self\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> sp.stats._multivariate.multivariate_normal_frozen:\n",
    "        assert self.x_train is not None and self.y_train is not None, \"Have to initialize the GP before calling\"\n",
    "        #\n",
    "        # ➡️ TODO : compute the posterior distribution ⬅️\n",
    "        #\n",
    "        # First, compute the posterior mean and covariance (use self.cov_inv and the definitions above), \n",
    "        # then compute the posterior distribution which is a multivariate normal distribution\n",
    "        # Hint: you might need to add a small diagonal value to the covariance matrix again\n",
    "        # Use the code from above for that (don't add more than 1e-6)\n",
    "\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "            \n",
    "        return ...\n",
    "\n",
    "    def posterior_mean(self, x: np.ndarray) -> np.ndarray:\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        dist = self(x)\n",
    "        return dist.mean\n",
    "\n",
    "    def posterior_covariance(self, x: np.ndarray) -> np.ndarray:\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        dist = self(x)\n",
    "        return dist.cov\n",
    "\n",
    "    def log_marginal_likelihood(\n",
    "            self,\n",
    "    ) -> float:\n",
    "        #\n",
    "        # ➡️ TODO : Implement this when you're instructed to in the notebook, you can skip this for now otherwise ⬅️\n",
    "        # \n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Safety check:** Run the following cell to make sure that everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:57:02.658731126Z",
     "start_time": "2023-05-08T11:57:02.579452197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(\n",
    "        GaussianProcess(mean, kern, np.random.rand()).initialize(np.random.rand(5, 1), np.random.rand(5, 1))(\n",
    "            np.random.rand(3, 1)\n",
    "        ),\n",
    "        sp.stats._multivariate.multivariate_normal_frozen\n",
    "):\n",
    "    print(\"✅ All good.\")\n",
    "else:\n",
    "    print(\"🚨 __call__ does not return a multivariate distribution. Make sure to use 'sp.stats.multivariate_normal'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializing and fitting the Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.451831504Z",
     "start_time": "2023-05-08T11:54:03.363909558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# ➡️ TODO : Create a new GP instance with a lengthscale of your choice. \n",
    "# Try different lengthscales to see how the affect the behavior of the GP.  ⬅️\n",
    "#       \n",
    "\n",
    "gp = ...\n",
    "gp.initialize(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.452056958Z",
     "start_time": "2023-05-08T11:54:03.364179772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_gp(gp: GaussianProcess, x_test: np.ndarray, y_test: np.ndarray, ax: plt.Axes = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot the GP posterior distribution of gp for a given set of test points (x_test and y_test).\n",
    "    Accepts an optional parameter ax which plots it on an existing ax, otherwise a new figure\n",
    "    is create.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    posterior_distribution = gp(x_test)\n",
    "    rvs = posterior_distribution.rvs(10)\n",
    "\n",
    "    ci_lb = []\n",
    "    ci_ub = []\n",
    "\n",
    "    for _x in x_test:\n",
    "        x_marginal = gp(_x)\n",
    "        _mean = x_marginal.mean.squeeze()\n",
    "        variance = x_marginal.cov.squeeze()\n",
    "\n",
    "        _lb, _ub = sp.stats.norm.interval(0.95, loc=_mean, scale=np.sqrt(variance))\n",
    "        ci_lb.append(_lb)\n",
    "        ci_ub.append(_ub)\n",
    "\n",
    "    ci_lb = np.array(ci_lb)\n",
    "    ci_ub = np.array(ci_ub)\n",
    "\n",
    "    for i, rv in enumerate(rvs):\n",
    "        x_test_sq = x_test.squeeze()\n",
    "        ax.plot(x_test.squeeze(), rv, alpha=0.25, color='blue', label='posterior sample' if i == 0 else None)\n",
    "    ax.fill_between(x_test.squeeze(), ci_lb, ci_ub, color='gray', alpha=0.5, label='95% CI')\n",
    "    ax.plot(x_test, y_test, color='red', label='f(x)')\n",
    "    ax.scatter(x_train, y_train, marker='x', color='black', label='training data')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the above GP for the test data defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.941063193Z",
     "start_time": "2023-05-08T11:54:03.406881598Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_gp(gp, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: play around with lengthscales\n",
    "\n",
    "* What happens if too low?\n",
    "* What happens if too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic approach to fit lengthscales: MLE\n",
    "\n",
    "We saw above that setting the lengthscale incorrectly can render the Gaussian process useless: if it is set too low or too high, the Gaussian process fails in modelling the function reasonably (if you didn't see that, try lengthscales of 0.1 and 10).\n",
    "While you probably found a lengthscale value that led to reasonable performance, we want to set the lengthscale automatically because what a \"good value\" is depends on the function at hand.\n",
    "All approaches to set the lengthscale (and often more hyperparameters) are in some form **maximizing the marginal likelihood of seeing the training data under the GP prior**:\n",
    "$$\n",
    "p(\\mathbf{y}|X,l) = \\int p(\\mathbf{y}|\\mathbf{f},X,l)p(\\mathbf{f}|X,l)d\\mathbf{f}\n",
    "$$\n",
    "We use the term _marginal_ likelihood because we are marginalizing over \"functions\" $\\mathbf{f}$.\n",
    "Note that $\\mathbf{f}$ is a $n$-dimensional vector which represents one possible realization of the function $f$ at the locations $X \\in \\mathbb{R}^{n\\times d}$.\n",
    "The vector $\\mathbf{f}|X$ follows a multivariate normal distribution, i.e., we marginalize over a normal distribution.\n",
    "\n",
    "We use the most straightforward way of learning the lengthscale: maximum likelihood estimation (MLE):\n",
    "$$\n",
    "\\hat{l} = \\arg\\max_l p(\\mathbf{y}|X,l)\n",
    "$$\n",
    "\n",
    "We state the posterior (log) marginal likelihood here and refer to [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf) for a derivation:\n",
    "$$\n",
    "\\log p(\\mathbf{f}|X,l) = -\\frac{1}{2}\\mathbf{f}^\\intercal K^{-1}\\mathbf{f}-\\frac{1}{2}\\log |K| -\\frac{n}{2}\\log 2\\pi\n",
    "$$\n",
    "\n",
    "\n",
    "**YOUR TASK**: Implement the `log_marginal_likelihood` in the `GP` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.941274568Z",
     "start_time": "2023-05-08T11:54:03.936674720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a function that we can pass to the minimize function from scikit learn\n",
    "\n",
    "def negative_marginal_log_likelihood(ls: float) -> float:\n",
    "    gp = GaussianProcess(mean, kern, ls)\n",
    "    gp.initialize(x_train, y_train)\n",
    "    return -gp.log_marginal_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize the marginal likelihood by minimizing the negative marginal log likelihood\n",
    "\n",
    "**Safety check:** The following cell should print something close to `[0.6862108]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.058849379Z",
     "start_time": "2023-05-08T11:54:03.939494104Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_ls = sp.optimize.minimize(negative_marginal_log_likelihood, 1)['x']\n",
    "print(best_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the GP with lengthscale fitted by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.627587887Z",
     "start_time": "2023-05-08T11:54:03.989725720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp = GaussianProcess(mean, kern, best_ls)\n",
    "gp.initialize(x_train, y_train)\n",
    "\n",
    "posterior_distribution = gp(x_test)\n",
    "\n",
    "plot_gp(gp, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Bayesian Optimization: Acquisition functions\n",
    "\n",
    "We now have all the ingredients of the Gaussian process and focus on the actual task: finding the optimizer of a function (i.e., the point with the best function value).\n",
    "The general strategy is to first fit a Gaussian process on a small number of initial training points and then find the next point to evaluate.\n",
    "\n",
    "We find this next point by maximizing a so-called _acquisition function_.\n",
    "While it may seem strange to solve an optimization problem (maximizing the acquisition function) to maximize our black-box function, the acquisition function can be maximized using gradient-based approaches because it has a closed-form expression.\n",
    "\n",
    "We will work with a popular acquisition function: __Expected improvement (EI)__.\n",
    "EI describes by how much, in expectation w.r.t. to our GP posterior after $n$ observations, a given point improves over current best function value observed so far $y^*_n$:\n",
    "$$\n",
    "EI_n(x) = \\mathbb{E}_{f\\sim GP(X,\\mathbf{y})}\\left[|f(x)-y^*_n| \\right]\n",
    "$$\n",
    "EI naturally does an _exploration-exploitation tradeoff_: points that have already been evaluated get zero EI (in noiseless models as in this labs) but regions where all possible functions have a value worse than the current best point also get zero EI. \n",
    "The sweet spot are regions that are under-explored but are also promising.\n",
    "\n",
    "Since our GP posterior follows a multivariate normal distribution, we can find a [closed form expression for the expected improvement](https://arxiv.org/pdf/1807.02811.pdf):\n",
    "$$\n",
    "EI_n(x) = \\left [\\Delta_n(x) \\right ]^+ + \\sigma_n(x)\\varphi \\left ( \\frac{\\Delta_n(x)}{\\sigma_n(x)} \\right ) - |\\Delta_n(x)|\\Phi \\left ( \\frac{\\Delta_n(x)}{\\sigma_n(x)} \\right )\n",
    "$$\n",
    "where $\\Delta_n(x) = \\mu_n(x)-y^*_n$ is the expected difference between the point $x$ and the best function values after $n$ observations $y^*_n$, $\\varphi(x)$ is the probability density function of the *standard* normal distribution, $\\Phi$ is the cummulative density function of the *standard* normal distribution, and $[x]^+:=\\max(0,x)$.\n",
    "The quantities $\\mu_n$ and $\\sigma_n$ are the posterior mean and posterior standard deviation after $n$ observations.\n",
    "\n",
    "__YOUR TASK:__ Implement the expected improvement acquisition function by adding missing lines in the cell below. You can use sp.stats for pds and cdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.632256151Z",
     "start_time": "2023-05-08T11:54:04.631077144Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expected_improvement(gp: GaussianProcess, x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim == 1:\n",
    "        x = x[np.newaxis, :]\n",
    "    eis = []\n",
    "    for _x in x:\n",
    "        # ➡️ TODO : Implement the expected improvement acquisition function by adding missing lines  ⬅️\n",
    "        ...\n",
    "        eis.append(_ei.squeeze())\n",
    "    return np.array(eis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Safety check:__ (make sure you executed all cells in order up to here, i.e., you used the GP with MLE). \n",
    "\n",
    "You should get the following figure:\n",
    "\n",
    "![EI plot](EI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:05.365253473Z",
     "start_time": "2023-05-08T11:54:04.634117019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(expected_improvement(gp, np.linspace(lb, ub, 300).reshape(-1, 1)).squeeze(), label=rf'$EI_n(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization\n",
    "\n",
    "Now, we have all the ingredients to do Bayesian optimization. We will \n",
    "* Sample some initial points and evaluate them\n",
    "* Fit a GP model with MLE\n",
    "* Find the next point to evaluate by maximizing the Expected improvement\n",
    "* Evaluate the next point, add it to the data and start over from the second point \n",
    "\n",
    "We will now see how all components play together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to maximize the likelihood\n",
    "\n",
    "We wrap our MLE procedure in a single function that we can call conveniently later on. Given `mean`, `kern`, `x_train`, and `y_train`, this function returns a GP where the lengthscale is set by MLE. \n",
    "\n",
    "**Question (after running the BO loop):** Why is the model of the function so accurate in the middle part of the function but poor in the outer parts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:05.365765649Z",
     "start_time": "2023-05-08T11:54:05.360057884Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximize_likelihood(\n",
    "        mean: Callable[[np.ndarray], np.ndarray],\n",
    "        kern: Callable[[np.ndarray], np.ndarray],\n",
    "        x_train: np.ndarray, y_train: np.ndarray\n",
    ") -> GaussianProcess:\n",
    "    gp = GaussianProcess(mean, kern, 0.1)\n",
    "    gp.initialize(x_train, y_train)\n",
    "\n",
    "    def likelihood_function(x: np.ndarray) -> float:\n",
    "        gp = GaussianProcess(mean, kern, x)\n",
    "        gp.initialize(x_train, y_train)\n",
    "        return -gp.log_marginal_likelihood()\n",
    "\n",
    "    best_ls = sp.optimize.minimize(likelihood_function, 1)['x']\n",
    "\n",
    "    gp = GaussianProcess(mean, kern, best_ls)\n",
    "    gp.initialize(x_train, y_train)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:19.081771031Z",
     "start_time": "2023-05-08T11:54:05.368826259Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.random.RandomState(2).rand(5) * (ub - lb) + lb\n",
    "x_train = x_train[:, np.newaxis]\n",
    "y_train = fn(x_train)\n",
    "\n",
    "gp = maximize_likelihood(mean, kern, x_train, y_train)\n",
    "ei = lambda x: expected_improvement(gp, x)\n",
    "\n",
    "for i in range(10):\n",
    "    print('###########################')\n",
    "    print(f\"####### iteration {i} #######\")\n",
    "    print('###########################')\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(7, 7))\n",
    "    # plot current GP\n",
    "    plot_gp(gp, x_test, y_test, ax=axs[0])\n",
    "    # stupidly optimize the acquisition function\n",
    "    x_range = np.linspace(lb, ub, 1000)[:, np.newaxis]\n",
    "    eis = ei(x_range)\n",
    "    x_next = x_range[np.argmax(eis)]\n",
    "\n",
    "    # plot acquisition function\n",
    "    axs[1].plot(x_range, eis)\n",
    "    axs[1].set_ylabel('EI(x)')\n",
    "    axs[1].vlines(x_next, ymin=ei(x_next), ymax=0, color='green', linestyle='dashed', label='best x')\n",
    "    axs[1].legend()\n",
    "    plt.show(fig)\n",
    "    # evaluate point where acquisition function maximum\n",
    "    fx_next = fn(x_next)\n",
    "    # add to training data\n",
    "    x_train = np.vstack((x_train, x_next[np.newaxis, :]))\n",
    "    y_train = np.vstack((y_train, fx_next[np.newaxis, :]))\n",
    "\n",
    "    gp = maximize_likelihood(mean, kern, x_train, y_train)\n",
    "    ei = lambda x: expected_improvement(gp, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PART 2\n",
    "***\n",
    "# Bayesian optimization in action\n",
    "\n",
    "Now, we'll use Bayesian optimization to tune the hyperparameters of a neural network.\n",
    "We will use the white wine dataset, which contains ~5.000 different wines with physical attributes and a taste score from users that goes from 1 to 10. Our goal is to train a predictor to guess the quality of the wine from its physical attributes. The data comes from the paper\n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "Modeling wine preferences by data mining from physicochemical properties.\n",
    "In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "However, we only use a subset of the data set to speed up training. We will use [BoTorch](https://botorch.org/) Bayesian optimization library to optimize the hyperparameters.\n",
    "\n",
    "![Wine plot](wine.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:13:06.340764Z",
     "start_time": "2024-03-19T14:13:06.328410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting the appropriate training device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:21.893921914Z",
     "start_time": "2023-05-08T11:54:20.300668225Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequality-white.csv\", delimiter=\";\")\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "\n",
    "# to speed up training we only use a subset of the data set. Feel free to play around. Especially if you have GPU support.\n",
    "n_data_samples = 50\n",
    "\n",
    "X = df.values[:n_data_samples, :-1]\n",
    "y = df.values[:n_data_samples, -1]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train).to(dtype=torch.double, device=device)\n",
    "X_test = torch.tensor(X_test).to(dtype=torch.double, device=device)\n",
    "y_train = torch.tensor(y_train).to(dtype=torch.double, device=device)\n",
    "y_test = torch.tensor(y_test).to(dtype=torch.double, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a relatively small ANN architecture with up to 4 hidden layers. We use dropout, with dropout probabilities that are subject to optimization. Furthermore, use ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:21.896721910Z",
     "start_time": "2023-05-08T11:54:21.894445535Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we define the general architecture for the ANN. It takes 3 parameter: number of hidden layers, \n",
    "# number of neurons per hidden layer and dropout probability.\n",
    "\n",
    "class ANN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden_layers: int,\n",
    "        hidden_width: int,\n",
    "        dropout_p: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(            \n",
    "            torch.nn.Linear(torch.tensor(11), hidden_width),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.ReLU(),\n",
    "            *(\n",
    "            torch.nn.Linear(hidden_width, hidden_width),                \n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.ReLU(),\n",
    "            ) * (n_hidden_layers),        \n",
    "            torch.nn.Linear(hidden_width, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimizer with a learning rate that is subject to optimization. We will use the Mean Squared Error loss function.\n",
    "\n",
    "To make the BO cleaner, we define the full training procedure in a function `black_box_function` that takes a vector of hyperparameters as input and returns the accuracy on the test set as output. We will use this function as a black box in the optimization. \n",
    "\n",
    "Note that the black_box_function takes inputs in the range [0,1] and then maps those to pre-set ranges inside the black_box_function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the entire training and evaluation loop in a function that we can optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.436243046Z",
     "start_time": "2023-05-08T11:54:23.434045709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def black_box_function(x: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    x[0]: number of hidden layers\n",
    "    x[1]: number of neurons per layer\n",
    "    x[2]: dropout p\n",
    "    x[3]: learning rate\n",
    "    x[4]: num training iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    n_hidden = int(torch.floor(x[0] * 4)) # maps from [0,1] -> {0,..,4}\n",
    "    n_neurons = int(torch.floor(x[1] * 996)) + 5 # maps from [0,1] -> {5, 500}\n",
    "    dropout_p = 0.1 * x[2] # maps from [0,1] -> [0, 0.1]\n",
    "    learning_rate = 0.1 * x[3] # maps from [0,1] -> [0, 0.1]\n",
    "    num_gd_iters = int(torch.floor(x[4] * 1901) + 100) # maps from [0,1] -> {100, 2000}\n",
    "\n",
    "    print(f'dropout: {dropout_p:.3E}', end='\\t')\n",
    "    print(f'lr: {learning_rate:.3E}', end='\\t')\n",
    "    print(f'depth: {n_hidden}', end='\\t')\n",
    "    print(f'width: {n_neurons}', end='\\t')\n",
    "    print(f'gd_iter: {num_gd_iters}')\n",
    "\n",
    "    model = ANN(int(n_hidden), int(n_neurons), dropout_p).to(dtype=torch.double, device=device)\n",
    "\n",
    "    #Defining the model hyperparameters\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate.item())\n",
    "\n",
    "    #Training process begins\n",
    "    model.train()\n",
    "    for epoch in range(num_gd_iters):\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn(y_pred.reshape(-1), y_train.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(X_train)\n",
    "        y_test_pred = model(X_test)\n",
    "        train_loss = loss_fn(y_train_pred.reshape(-1), y_train.reshape(-1))\n",
    "        test_loss = loss_fn(y_test_pred.reshape(-1), y_test.reshape(-1))\n",
    "        print(f\"Test loss = {test_loss:.3f}\", f\"[Training loss = {train_loss:.3f}]\")\n",
    "        \n",
    "\n",
    "        # We maximize the negative test loss, i.e., minimize the loss\n",
    "        return -test_loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We test that the black box function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.436243046Z",
     "start_time": "2023-05-08T11:54:23.434045709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "l = black_box_function(torch.tensor([1, 1,  0, .01, .1], dtype=torch.double, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the GP model\n",
    "\n",
    "We model the GP with GPyTorch. The Matern kernel is similar to the RBF kernel we used above. We have not talked about `ScaleKernel`s but they allow the model to adapt to the variance of the values of $f$. Finally, GPs can also model noisy functions. We haven't talked about noise above but the definitions of a noisy GP are almost identical to the ones of noiseless GPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.617987989Z",
     "start_time": "2023-05-08T11:54:23.438063797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "\n",
    "\n",
    "def get_gp(x_train: torch.Tensor, y_train: torch.Tensor) -> SingleTaskGP:\n",
    "    # ➡️ TODO : Create a new SingleTaskGP and return the model  ⬅️\n",
    "    # See https://botorch.org/api/_modules/botorch/models/gp_regression.html#SingleTaskGP for hints\n",
    "    # You can try to improve your GP by choosing another kernel, another length scale prior, or \n",
    "    # another prior for the likelihood. The default should be fine though.\n",
    "\n",
    "    model = ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheap function to test if everything works\n",
    "\n",
    "You can use the following test function to check if your BO loop runs as intended before running on the expensive HPO problem. On the Branin problem, you should reach a value of around -0.4 after approximately 200 function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from botorch.test_functions import Branin\n",
    "\n",
    "cheap_function = Branin(negate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a space-filling criterion (\"scrambled Sobol sequence\") to draw the initial points. \n",
    "This basically ensures that we sample evenly from the space, i.e., don't sample more from certain regions of the space than from others.\n",
    "As above, we use Expected Improvement as the acquisition function. We will optimize the acquisition function using the L-BFGS algorithm. The Bayesian optimization loop will run for 24 iterations (and might take some time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:56:17.210579536Z",
     "start_time": "2023-05-08T11:54:23.614538186Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.optim import optimize_acqf_mixed\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from torch.quasirandom import SobolEngine\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "\n",
    "# Since some of our parameters are integer values, we need to ensure that we only evaluate the acquistion\n",
    "# function at ``legal'' positions. We first define the feasible positions in the [0, 1] space and create\n",
    "# a list of dictionaries that will be useful later\n",
    "options_n_hidden = np.arange(4) / 4\n",
    "options_n_neurons = (np.arange(5,501) - 5) / 501\n",
    "options_gd_iters = (np.arange(100,2001) - 100) / 1901\n",
    "\n",
    "fixed_features_list = []\n",
    "# iterate over the cartesian product of feasible locations and add a dictionary entry for each\n",
    "for _n_hid, _n_neur, _n_gd_it in itertools.product(options_n_hidden, options_n_neurons, options_gd_iters):\n",
    "    fixed_features_list.append({\n",
    "        0: _n_hid, # 0: index of 'n_hidden'\n",
    "        1: _n_neur, # 1: index of 'n_neurons'\n",
    "        4: _n_gd_it # 2: # 0: index of 'n_gd_iters'\n",
    "    })\n",
    "n_fixed_features = len(fixed_features_list) # this is a rather long list \n",
    "\n",
    "# ➡️ TODO : Set this to 'cheap_function' or 'black_box_function' ⬅️\n",
    "function_to_optimize = ...\n",
    "dim = 2 if isinstance(function_to_optimize, Branin) else 5\n",
    "\n",
    "# draw d+1 initial points, replace values for discrete parameters with random values from our list\n",
    "x_init = SobolEngine(dim, scramble=True).draw(dim+1)\n",
    "for i_init in range(dim+1):\n",
    "    fixed_features = fixed_features_list[np.random.choice(n_fixed_features)]\n",
    "    for x_idx, x_val in fixed_features.items():\n",
    "        x_init[i_init, x_idx] = x_val\n",
    "    \n",
    "fx_init = torch.tensor([function_to_optimize(x) for x in x_init]).reshape(-1, 1)\n",
    "\n",
    "x = x_init\n",
    "fx = fx_init\n",
    "\n",
    "# Set to 500 for cheap function and to 24 for the HPO problem.\n",
    "N_BO_STEPS  = 500 if isinstance(function_to_optimize, Branin) else 24\n",
    "# Set to 1 for the HPO problem\n",
    "PRINT_EVERY = 50 if isinstance(function_to_optimize, Branin) else 1\n",
    "\n",
    "\n",
    "print('*** Starting Bayesian Optimization ***')\n",
    "for gp_iter in range(N_BO_STEPS):\n",
    "    if gp_iter % PRINT_EVERY == 0:\n",
    "        print(f'** Iteration {gp_iter + 1}/{N_BO_STEPS} **')\n",
    "    # We define the bounds for the optimization. We assume that all hyperparameters are between 0 and 1.\n",
    "    bounds = torch.stack([torch.zeros(dim), torch.ones(dim)]) if not isinstance(function_to_optimize, Branin) else torch.stack([torch.tensor([-5, 0]), torch.tensor([10,15])])\n",
    "    \n",
    "    # ➡️ TODO : Normalize the x values to be between 0 and 1. You may use the botorch transforms  ⬅️\n",
    "    # https://botorch.org/api/_modules/botorch/utils/transforms.html\n",
    "    x_train = ...\n",
    "    \n",
    "    # ➡️ TODO : Normalize the y values to have mean zero and standard deviation one.  ⬅️\n",
    "    fx_train = ...\n",
    "    \n",
    "    # ➡️ TODO : Create a new GP with the normalized x and y values (use your get_gp function from above) ⬅️\n",
    "    gp = ...\n",
    "    \n",
    "    # Your GP model has an attribute `likelihood` which you can use to compute the marginal log likelihood.\n",
    "    # This attribute gives the term p(y|f,X,l) in the marginal log likelihood which in our case\n",
    "    # is a Gaussian (see https://docs.gpytorch.ai/en/stable/likelihoods.html#gaussianlikelihood )\n",
    "    # ➡️ TODO : Define the marginal log likelihood of the model (see https://docs.gpytorch.ai/en/stable/marginal_log_likelihoods.html#exactmarginalloglikelihood ) ⬅️\n",
    "    # ➡️ TODO : Train the model (see https://botorch.org/api/fit.html) ⬅️\n",
    "    mll = ...\n",
    "\n",
    "    # ➡️ TODO : fit the model by maximizing the marginal likelihood (see https://botorch.org/api/fit.html#botorch.fit.fit_gpytorch_mll)\n",
    "    ...\n",
    "    \n",
    "    # ➡️ TODO : Define an acquisition function for your model. We'll use the Expected Improvement (see https://botorch.org/api/_modules/botorch/acquisition/analytic.html#ExpectedImprovement )  ⬅️\n",
    "    ei = ...\n",
    "\n",
    "    # we shuffle our list of values for discrete parameters and only keep the first 100 to keep a reasonably fast optimization\n",
    "    random.shuffle(fixed_features_list)\n",
    "    fixed_features_list_subsample = fixed_features_list[:100] \n",
    "\n",
    "    # ➡️ TODO : Optimize the acquisition function. You can use the optimize_acqf function (see https://botorch.org/api/optim.html#botorch.optim.optimize.optimize_acqf_mixed ) ⬅️\n",
    "    # Set the number of candidates to 1, num restarts to 3, and raw_samples to 1024 , pass fixed_features_list_subsample for the fixed features\n",
    "    \n",
    "    x_next, acq_value = optimize_acqf_mixed(\n",
    "        ...\n",
    "    )\n",
    "    \n",
    "    # We unnormalize x_next to be in the original bounds\n",
    "    x_next_unnorm = unnormalize(x_next, bounds).detach().cpu()\n",
    "\n",
    "    # Lastly we evaluate your black-box function on the point suggested by the acquisition function\n",
    "    fx_next = function_to_optimize(x_next_unnorm.squeeze())\n",
    "    \n",
    "    if gp_iter % PRINT_EVERY == 0:\n",
    "        print(f'New function value: {fx_next.item():.4f}. Current best: {fx.max().item():.4f}')\n",
    "        print('\\n')\n",
    "    x = torch.cat([x, x_next_unnorm])\n",
    "    fx = torch.cat([fx, fx_next.reshape(-1, 1)])\n",
    "\n",
    "x_bo = x\n",
    "fx_bo = fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing BO to random search\n",
    "\n",
    "We compare the performance of the models found by BO to randomly searching for ANN hyperparameters. Normally, we see that BO outperforms random search. Sometimes more, sometimes less. How does it perform in this case?\n",
    "\n",
    "__Your task:__ Randomly initialize 30 CNNs and evaluate their performance. The performances should be in the 1D tensor `fx_rs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.646792551Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ➡️ TODO : Randomly initialize 30 ANNs and evaluate their performance.  ⬅️\n",
    "\n",
    "x_rs = ...\n",
    "fx_rs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the evolution of performance\n",
    "\n",
    "Assuming that we have sequential setting where we run one configuration at a time, we can plot the accuracy of best configuration seen so far against time. This way, the y value for the two methods state the performacen we would have stopped after x iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.652955668Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fx_bo_max = np.minimum.accumulate(-fx_bo)\n",
    "fx_rs_max = np.minimum.accumulate(-fx_rs)\n",
    "\n",
    "plt.plot(fx_bo_max, label='Bayesian Optimization')\n",
    "plt.plot(fx_rs_max, label='Random Search')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you want to, you can reuse this BO loop for your other machine learning projects, just replace the black box function for whatever you want to optimize. :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
